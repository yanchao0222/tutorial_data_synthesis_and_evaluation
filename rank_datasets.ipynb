{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b00d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e48e3",
   "metadata": {},
   "source": [
    "# 1. Load ranks for individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6553ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_rank = np.load('./result/utility_score_dataset_ranks.npy', allow_pickle=True).item()\n",
    "privacy_rank = np.load('./result/privacy_score_dataset_ranks.npy', allow_pickle=True).item()\n",
    "RUN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b41b17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dimension-wise distribution': [3, 1, 5, 4, 2],\n",
       " 'Column-wise correlation': [2, 1, 3, 5, 4],\n",
       " 'Latent cluster analysis': [2, 1, 3, 5, 4],\n",
       " 'Medical concept abundance': [3, 2, 5, 4, 1],\n",
       " 'Clinical knowledge violation': [2, 4, 1, 3, 5],\n",
       " 'TSTR_auroc': [2, 1, 3, 5, 4],\n",
       " 'TRTS_auroc': [2, 1, 3, 5, 4],\n",
       " 'Feature importance': [2, 1, 4, 4, 4]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62491f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Membership_inference_risk': [3, 5, 2, 1, 4],\n",
       " 'Attribute_inference_risk': [3, 2, 4, 1, 5]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacy_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78f5469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dimension-wise distribution': [3, 1, 5, 4, 2],\n",
       " 'Column-wise correlation': [2, 1, 3, 5, 4],\n",
       " 'Latent cluster analysis': [2, 1, 3, 5, 4],\n",
       " 'Medical concept abundance': [3, 2, 5, 4, 1],\n",
       " 'Clinical knowledge violation': [2, 4, 1, 3, 5],\n",
       " 'TSTR_auroc': [2, 1, 3, 5, 4],\n",
       " 'TRTS_auroc': [2, 1, 3, 5, 4],\n",
       " 'Feature importance': [2, 1, 4, 4, 4],\n",
       " 'Membership_inference_risk': [3, 5, 2, 1, 4],\n",
       " 'Attribute_inference_risk': [3, 2, 4, 1, 5]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_all_metrics = {**utility_rank, **privacy_rank}\n",
    "rank_all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851ab47",
   "metadata": {},
   "source": [
    "# 2. Specify weight profile for individual metrics given a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3107ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ranks:  [2.3, 1.8, 3.2000000000000006, 3.7, 3.9999999999999996]\n",
      "Dataset 1 is the best\n"
     ]
    }
   ],
   "source": [
    "weight_profile = {}\n",
    "weight_profile['Dimension-wise distribution'] = 0.1\n",
    "weight_profile['Column-wise correlation'] = 0.1\n",
    "weight_profile['Latent cluster analysis'] = 0.1\n",
    "weight_profile['Medical concept abundance'] = 0.0\n",
    "weight_profile['Clinical knowledge violation'] = 0.1\n",
    "weight_profile['TSTR_auroc'] = 0.2\n",
    "weight_profile['TRTS_auroc'] = 0.0\n",
    "weight_profile['Feature importance'] = 0.2 \n",
    "\n",
    "weight_profile['Membership_inference_risk'] = 0.1\n",
    "weight_profile['Attribute_inference_risk'] = 0.1\n",
    "\n",
    "final_ranks = []\n",
    "for i in range(RUN):\n",
    "    rank_value = 0\n",
    "    for key, weight in weight_profile.items():\n",
    "        rank_value += weight * rank_all_metrics[key][i]\n",
    "    final_ranks.append(rank_value)\n",
    "print('Final ranks: ', final_ranks)\n",
    "print('Dataset %d is the best' % final_ranks.index(min(final_ranks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba7796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ranks:  [2.5, 3.1999999999999997, 2.4000000000000004, 2.5000000000000004, 4.4]\n",
      "Dataset 2 is the best\n"
     ]
    }
   ],
   "source": [
    "weight_profile = {}\n",
    "weight_profile['Dimension-wise distribution'] = 0.1\n",
    "weight_profile['Column-wise correlation'] = 0.1\n",
    "weight_profile['Latent cluster analysis'] = 0.0\n",
    "weight_profile['Medical concept abundance'] = 0.0\n",
    "weight_profile['Clinical knowledge violation'] = 0.4\n",
    "weight_profile['TSTR_auroc'] = 0.0\n",
    "weight_profile['TRTS_auroc'] = 0.0\n",
    "weight_profile['Feature importance'] = 0.0 \n",
    "\n",
    "weight_profile['Membership_inference_risk'] = 0.2\n",
    "weight_profile['Attribute_inference_risk'] = 0.2\n",
    "\n",
    "final_ranks = []\n",
    "for i in range(RUN):\n",
    "    rank_value = 0\n",
    "    for key, weight in weight_profile.items():\n",
    "        rank_value += weight * rank_all_metrics[key][i]\n",
    "    final_ranks.append(rank_value)\n",
    "print('Final ranks: ', final_ranks)\n",
    "print('Dataset %d is the best' % final_ranks.index(min(final_ranks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e353766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
