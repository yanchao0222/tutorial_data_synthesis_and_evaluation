{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85340848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import wasserstein_distance\n",
    "from itertools import chain\n",
    "from numpy import linalg as LA\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import shap\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score, confusion_matrix\n",
    "import scipy.stats as ss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea7ba9",
   "metadata": {},
   "source": [
    "# Load real and synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load real data\n",
    "real_data_df = pd.read_csv('/YOUR_LOCAL_PATH/original_training_data.csv') # note this is the training data for GAN training (not include the test data)\n",
    "min_max_log = np.load('/YOUR_LOCAL_PATH/min_max_log.npy', allow_pickle=True).item()\n",
    "# for key, min_max in min_max_log.items():\n",
    "#     min_, max_ = min_max[0], min_max[1]\n",
    "#     col_values = np.array(real_data_df[key])\n",
    "#     real_data_df[key] = (1 - col_values)*max_ + col_values*min_\n",
    "condition_columns = list(real_data_df.columns)[8:-4]\n",
    "# real_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade72775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(real_data_df['WHITE']))\n",
    "print(np.sum(real_data_df['BLACK']))\n",
    "print(np.sum(real_data_df['ASIAN']))\n",
    "print(np.sum(real_data_df['HISPANIC']))\n",
    "print(np.sum(real_data_df['UN']))\n",
    "print(np.sum(real_data_df['OTHER']))\n",
    "print('Num of y-positive record %d' % np.sum(real_data_df['DIE_1y']))\n",
    "print('Num of y-negative record %d' % (len(real_data_df) - np.sum(real_data_df['DIE_1y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic data\n",
    "col_name_list = list(real_data_df.columns)\n",
    "# model_id_list = [11,12,13,14,15]\n",
    "# ckpt_id_list = [38,38,38,38,38]\n",
    "model_id_list = [21,22,23,24,25]\n",
    "ckpt_id_list = ['best','best','best','best','best']\n",
    "# model_id_list = ['xxx']\n",
    "# ckpt_id_list = [229]\n",
    "syn_data_list = []\n",
    "for model_id, ckpt_id in zip(model_id_list, ckpt_id_list):\n",
    "    \n",
    "    syn_data = np.load(f'/YOUR_LOCAL_PATH/GAN_training/syn/emrwgan_model_{model_id}_ckpt_{ckpt_id}.npy', allow_pickle=True)\n",
    "    for i in range(len(col_name_list)-4):\n",
    "        syn_data[:,i] = (syn_data[:,i] >= 0.5)*1.0\n",
    "    syn_data_df = pd.DataFrame(syn_data, columns = col_name_list)\n",
    "    positive_outcome_syn_data = syn_data_df[syn_data_df['DIE_1y'] == 1.0].values\n",
    "    negative_outcome_syn_data = syn_data_df[syn_data_df['DIE_1y'] == 0.0].values\n",
    "    syn_data_df = np.concatenate((positive_outcome_syn_data[:14243,:], negative_outcome_syn_data[:112279,:]), axis=0)\n",
    "    syn_data_df = pd.DataFrame(syn_data_df, columns = col_name_list)\n",
    "    \n",
    "    syn_data_list.append(syn_data_df.values)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(syn_data_df['WHITE']))\n",
    "print(np.sum(syn_data_df['BLACK']))\n",
    "print(np.sum(syn_data_df['ASIAN']))\n",
    "print(np.sum(syn_data_df['HISPANIC']))\n",
    "print(np.sum(syn_data_df['UN']))\n",
    "print(np.sum(syn_data_df['OTHER']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score = {}\n",
    "privacy_score = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f820a",
   "metadata": {},
   "source": [
    "# Data utility evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2bcd71",
   "metadata": {},
   "source": [
    "## 1. Utility Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c738191",
   "metadata": {},
   "source": [
    "### 1.a Dimension-wise distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc076136",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimension-wise distribution: binary features\n",
    "\n",
    "binary_d2d_average = {}\n",
    "binary_d2d_sum = {}\n",
    "binary_perc_real = []\n",
    "FEATURE_COUNT = len(syn_data_df.columns) - 4\n",
    "train_data = real_data_df.values\n",
    "for c in range(0,FEATURE_COUNT):\n",
    "    if c != 6:\n",
    "        binary_perc_real.append(np.sum(train_data[:,c])/train_data.shape[0])\n",
    "print(len(binary_perc_real))\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize = (32,5.5))\n",
    "plt.setp(axs, xticks=[0, 1.0, 1],\n",
    "        yticks=[0, 1.0, 1])\n",
    "\n",
    "# plt.xticks(fontsize=20)\n",
    "\n",
    "sd2d_result = []\n",
    "ad2d_result = []\n",
    "for run in range(len(syn_data_list)):\n",
    "    syn_data = syn_data_list[run]\n",
    "    binary_perc_syn = []\n",
    "\n",
    "    for c in range(0,FEATURE_COUNT):\n",
    "        if c != 6:\n",
    "            binary_perc_syn.append(np.sum(syn_data[:,c])/train_data.shape[0])\n",
    "    # define categories for features\n",
    "    cluster_list= np.array([0]*6 + [2] + [1]*len(condition_columns))\n",
    "    # calculate sd2d for each synthetic data\n",
    "    sd2d = np.sum(abs(np.array(binary_perc_syn) - np.array(binary_perc_real)))\n",
    "    ad2d = np.sum(abs(np.array(binary_perc_syn) - np.array(binary_perc_real))) / FEATURE_COUNT * 1000.0 \n",
    "\n",
    "    for index in [2,1,0]:\n",
    "        if index == 0:\n",
    "            marker = \"s\"\n",
    "            color = 'black'\n",
    "            label = 'Race'\n",
    "        elif index == 1:\n",
    "            marker = \"o\"\n",
    "            color = 'pink'\n",
    "            label = 'Diagnosis'\n",
    "        else:\n",
    "            marker = \"d\"\n",
    "            color = 'green'\n",
    "            label = 'Gender'\n",
    "\n",
    "        axs[run].scatter(np.array(binary_perc_real)[cluster_list==index], np.array(binary_perc_syn)[cluster_list==index], color = color, s = 48, label=label,marker = marker,alpha=0.9)\n",
    "\n",
    "\n",
    "    axs[run].set_xlabel(\"Real\", fontsize = 20)\n",
    "    axs[run].set_ylabel(\"Synthetic\", fontsize = 20)\n",
    "    axs[run].set_xlim(0, 1)\n",
    "    axs[run].set_ylim(0, 1)\n",
    "    axs[run].tick_params(labelsize=20)\n",
    "\n",
    "    #axs[i,run-1].set_title(f'{syn_list[i]}_{run}', fontsize = 8)\n",
    "    axs[run].plot([0, 1], [0, 1], ls=\"--\", c=\".1\")\n",
    "    axs[run].text(0.4, 0.10, f'APD = {ad2d:.2f}', fontsize = 20)\n",
    "    sd2d_result.append(sd2d)\n",
    "    ad2d_result.append(ad2d)\n",
    "\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize=15, frameon=False)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize=20, frameon=False)\n",
    "plt.show()\n",
    "# fig.savefig('./gan_com_5run_vumc_0630.eps',bbox_inches='tight',format='eps') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da75a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd2d_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90296b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimension-wise distribution: continuous features\n",
    "\n",
    "print(np.min(list(real_data_df['AGE'])))\n",
    "print(np.max(list(real_data_df['AGE'])))\n",
    "\n",
    "print(np.min(list(real_data_df['BMI'])))\n",
    "print(np.max(list(real_data_df['BMI'])))\n",
    "\n",
    "print(np.min(list(real_data_df['DIASTOLIC'])))\n",
    "print(np.max(list(real_data_df['DIASTOLIC'])))\n",
    "\n",
    "print(np.min(list(real_data_df['SYSTOLIC'])))\n",
    "print(np.max(list(real_data_df['SYSTOLIC'])))\n",
    "\n",
    "continuous_w_d = {}\n",
    "continuous_columns = ['AGE', 'BMI', 'DIASTOLIC', 'SYSTOLIC']\n",
    "\n",
    "RUN_list = [1,2,3,4,5]\n",
    "\n",
    "for column in continuous_columns:\n",
    "    distances_to_real = []\n",
    "\n",
    "    model_list_figure = ['Real']\n",
    "    real_values = list(real_data_df[column])\n",
    "    data_list = [real_values]\n",
    "    color_list = ['maroon']\n",
    "    for run in RUN_list:\n",
    "        emrwgan_df = pd.DataFrame(syn_data_list[run-1], columns = list(real_data_df.columns))\n",
    "        data_list.append(list(emrwgan_df[column]))\n",
    "        color_list.append('blue')\n",
    "        model_list_figure.append('EMR-WGAN')\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 3))\n",
    "    \n",
    "    print('\\n %s' % column)\n",
    "    for subplot_count in range(len(data_list)):\n",
    "        \n",
    "        if subplot_count > 0:\n",
    "            distances_to_real.append(wasserstein_distance(real_values, data_list[subplot_count]))\n",
    "        \n",
    "        plt.subplot(1, 6, subplot_count+1)\n",
    "        # fig = plt.figure(figsize = (3, 5))\n",
    "        plt.hist(data_list[subplot_count], color = color_list[subplot_count])\n",
    "        plt.xlabel(\"Value\")\n",
    "#         plt.ylim(0, 10000)\n",
    "        plt.title(model_list_figure[subplot_count])\n",
    "        plt.grid(color = 'blue', linestyle = '--', axis = 'y')\n",
    "\n",
    "    plt.show()\n",
    "    print(\"EMR-WGAN : mean: %.4f, std: %.4f\" % (np.mean(distances_to_real), np.std(distances_to_real)))\n",
    "\n",
    "    continuous_w_d[column] = distances_to_real\n",
    "print(continuous_w_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_value = {}\n",
    "norm_continuous_value = {}\n",
    "    \n",
    "for test, w_d in continuous_w_d.items():\n",
    "    w_d = list(w_d)\n",
    "    max_val = np.max(w_d)\n",
    "    min_val = np.min(w_d)\n",
    "    norm_w_d = (w_d - min_val) / (max_val-min_val) / 10\n",
    "    continuous_value[test] = w_d\n",
    "    norm_continuous_value[test] = list(norm_w_d)\n",
    "    \n",
    "continuous_value_df = pd.DataFrame.from_dict(continuous_value)\n",
    "norm_continuous_value_df = pd.DataFrame.from_dict(norm_continuous_value)\n",
    "sum_all_continuous_variables = np.sum(norm_continuous_value_df.values, axis = 1)\n",
    "print(sum_all_continuous_variables)\n",
    "norm_continuous_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_score_binary_cont = []\n",
    "for run in RUN_list:\n",
    "    combine_score_binary_cont.append((sd2d_result[run-1] + sum_all_continuous_variables[run-1])/(len(real_data_df.columns)-1) * 1000)\n",
    "\n",
    "print('Dimension-wise distribution scores for all considered runs:', combine_score_binary_cont)\n",
    "utility_score['Dimension-wise distribution'] = combine_score_binary_cont\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bb6cc",
   "metadata": {},
   "source": [
    "### 1.b Column-wise correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cor = np.corrcoef(np.transpose(train_data))\n",
    "cwc = []\n",
    "print(\"Column-wise correlation difference: \\n\")\n",
    "for matrix in syn_data_list:\n",
    "    np.random.seed(0)\n",
    "    noise_matrix = (np.random.rand(len(matrix),len(matrix[0])) - 1) / 100000000    \n",
    "    syn_cor = np.corrcoef(np.transpose(np.array(matrix) + noise_matrix))\n",
    "    cwc.append(LA.norm(real_cor - syn_cor, 'fro') / len(real_data_df.columns) / len(real_data_df.columns) * 1000 * 1000)\n",
    "\n",
    "print(cwc)\n",
    "utility_score['Column-wise correlation'] = cwc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24b89f",
   "metadata": {},
   "source": [
    "### 1.c Latent cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_data_tmp = copy.deepcopy(train_data)\n",
    "train_data_df = pd.DataFrame(train_data_tmp)\n",
    "train_data_df.loc[:,[1456, 1457, 1458, 1459]] = min_max_scaler.fit_transform(train_data_df[[1456, 1457, 1458, 1459]].values)\n",
    "NUM_C = 5 ## the number of clusters which has been optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cedfa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(real_data_df['AGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f44ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cluster_score_list = []\n",
    "print(\"Latent deviation: \\n\")\n",
    "for matrix in syn_data_list:\n",
    "    matrix_df = pd.DataFrame(copy.deepcopy(matrix))\n",
    "    matrix_df.loc[:,[1456, 1457, 1458, 1459]] = min_max_scaler.fit_transform(matrix_df[[1456, 1457, 1458, 1459]].values)\n",
    "    mixed_data = np.concatenate((train_data_df.values,matrix_df.values), axis = 0)\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(mixed_data)\n",
    "    sum_diag = np.sum(pca.explained_variance_ratio_)\n",
    "    i = 1\n",
    "    while  np.sum(pca.explained_variance_ratio_[:i]) < 0.8:\n",
    "        i += 1\n",
    "#     print(i, np.sum(pca.explained_variance_ratio_[:i])/np.sum(pca.explained_variance_ratio_))\n",
    "    pca = PCA(n_components=i)\n",
    "    pca_result = pca.fit_transform(mixed_data)\n",
    "    \n",
    "    kmeans_model = KMeans(n_clusters=NUM_C).fit(pca_result)\n",
    "    cluster_aff = kmeans_model.labels_.tolist()\n",
    "    real_syn_label = [1]*len(train_data_df.values) + [0]*len(matrix_df.values)\n",
    "    \n",
    "    cluster_score_sum = 0\n",
    "    for label in range(NUM_C):\n",
    "        indices_label = [i for i in range(len(cluster_aff)) if cluster_aff[i] == label]\n",
    "        real_syn_for_label = [real_syn_label[i] for i in indices_label]\n",
    "        ratio = np.sum(real_syn_for_label)/len(real_syn_for_label)\n",
    "        cluster_score_sum += (ratio - 0.5)**2\n",
    "    log_cluster_score_list.append(math.log2(cluster_score_sum/NUM_C))\n",
    "print(log_cluster_score_list)\n",
    "utility_score['Latent cluster analysis'] = log_cluster_score_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c18fb",
   "metadata": {},
   "source": [
    "### 1.d Medical concept abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa987409",
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_train_data_df = real_data_df[condition_columns]\n",
    "mca_train_data = np.sum(mca_train_data_df.values, axis=1)\n",
    "n, bins, patches = plt.hist(x=mca_train_data, bins=20, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "mca_syn_data_list = []\n",
    "\n",
    "for matrix in syn_data_list:\n",
    "    syn_data_df = pd.DataFrame(data = matrix, columns = list(real_data_df.columns))\n",
    "    mca_syn_data_df = syn_data_df[condition_columns]\n",
    "    mca_syn_data = np.sum(mca_syn_data_df.values, axis=1).astype(int)\n",
    "    count_in_bins = {}\n",
    "\n",
    "    bin_counts = [0]*len(n) \n",
    "    for data_point in mca_syn_data:\n",
    "        bin_number = data_point // (bins[1]-bins[0])\n",
    "        if bin_number >= len(n):\n",
    "            bin_counts[-1] += 1\n",
    "        else:\n",
    "            bin_counts[int(bin_number)] += 1\n",
    "\n",
    "    mca_syn_data_list.append(np.sum(np.abs(np.array(bin_counts)-n))*0.5/len(mca_train_data))\n",
    "\n",
    "print(\"Medical concept abundance distances:\", mca_syn_data_list)\n",
    "utility_score['Medical concept abundance'] = mca_syn_data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f9ec1",
   "metadata": {},
   "source": [
    "### 1.e Clinical knowledge violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29322dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_male_dict = {}\n",
    "disease_female_dict = {}\n",
    "\n",
    "gender_column = real_data_df['GENDER'].tolist()\n",
    "for column in condition_columns:\n",
    "    disease_column = real_data_df[column].tolist()\n",
    "    patient_positive = [index for index in range(len(disease_column)) if disease_column[index] == 1]\n",
    "    gender_positive_patient = [gender_column[index] for index in patient_positive]\n",
    "    if np.sum(gender_positive_patient) == 0: # male\n",
    "        disease_male_dict[column] = len(patient_positive)\n",
    "    if np.sum(gender_positive_patient) == len(gender_positive_patient): # female\n",
    "        disease_female_dict[column] = len(patient_positive)\n",
    "\n",
    "    \n",
    "sorted_disease_female_dict = sorted(disease_female_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_disease_male_dict = sorted(disease_male_dict.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# male: 600: Hyperplasia of prostate\n",
    "# male: 185: Cancer of prostate\n",
    "# male: 605: Erectile dysfunction [ED]\n",
    "\n",
    "# female: 649: Other conditions or status of the mother complicating pregnancy, childbirth, or the puerperium\n",
    "# female: 655: Known or suspected fetal abnormality affecting management of mother\n",
    "# female: 646: Other complications of pregnancy NEC\n",
    "\n",
    "print('    Male codes: ')\n",
    "for code in ['600', '185', '605']:\n",
    "    disease_column = real_data_df[code].tolist()\n",
    "    patient_positive = [index for index in range(len(disease_column)) if disease_column[index] == 1]\n",
    "    gender_column = real_data_df['GENDER'].tolist()\n",
    "    gender_positive_patient = [gender_column[index] for index in patient_positive]\n",
    "    print('       ' + code + ': # total patients = ' + str(np.sum(disease_column)) + '; male percentage: ' + str((len(gender_positive_patient) - np.sum(gender_positive_patient))/len(gender_positive_patient)) + '; female percentage: ' + str(np.sum(gender_positive_patient)/len(gender_positive_patient)))\n",
    "\n",
    "print('    Female codes: ')\n",
    "for code in ['649', '655', '646']:\n",
    "    disease_column = real_data_df[code].tolist()\n",
    "    patient_positive = [index for index in range(len(disease_column)) if disease_column[index] == 1]\n",
    "    gender_column = real_data_df['GENDER'].tolist()\n",
    "    gender_positive_patient = [gender_column[index] for index in patient_positive]\n",
    "    print('       ' + code + ': # total patients = ' + str(np.sum(disease_column)) + '; male percentage: ' + str((len(gender_positive_patient) - np.sum(gender_positive_patient))/len(gender_positive_patient)) + '; female percentage: ' + str(np.sum(gender_positive_patient)/len(gender_positive_patient)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 1\n",
    "male_code_violation = []\n",
    "female_code_violation = []\n",
    "for matrix in syn_data_list:\n",
    "    syn_data_df = pd.DataFrame(data = matrix, columns = list(real_data_df.columns))\n",
    "    print('\\nSynthetic data ' + str(run) + ':')\n",
    "    print('    Male codes: ')\n",
    "    male_viol_sum = 0\n",
    "    for code in ['600', '185', '605']:\n",
    "        disease_column = syn_data_df[code].tolist()\n",
    "        patient_positive = [index for index in range(len(disease_column)) if disease_column[index] == 1]\n",
    "        gender_column = syn_data_df['GENDER'].tolist()\n",
    "        gender_positive_patient = [gender_column[index] for index in patient_positive]\n",
    "        print('       ' + code + ': # total patients = ' + str(np.sum(disease_column)) + '; male percentage: ' + str((len(gender_positive_patient) - np.sum(gender_positive_patient))/len(gender_positive_patient)) + '; female percentage: ' + str(np.sum(gender_positive_patient)/len(gender_positive_patient)))\n",
    "        male_viol_sum += np.sum(gender_positive_patient)/len(gender_positive_patient)\n",
    "    male_code_violation.append(male_viol_sum/3)\n",
    "    \n",
    "    print('    Female codes: ')\n",
    "    female_viol_sum = 0\n",
    "    for code in ['649', '655', '646']:\n",
    "        disease_column = syn_data_df[code].tolist()\n",
    "        patient_positive = [index for index in range(len(disease_column)) if disease_column[index] == 1]\n",
    "        gender_column = syn_data_df['GENDER'].tolist()\n",
    "        gender_positive_patient = [gender_column[index] for index in patient_positive]\n",
    "        print('       ' + code + ': # total patients = ' + str(np.sum(disease_column)) + '; male percentage: ' + str((len(gender_positive_patient) - np.sum(gender_positive_patient))/len(gender_positive_patient)) + '; female percentage: ' + str(np.sum(gender_positive_patient)/len(gender_positive_patient)))\n",
    "        female_viol_sum += (len(gender_positive_patient) - np.sum(gender_positive_patient))/len(gender_positive_patient)\n",
    "    female_code_violation.append(female_viol_sum/3)\n",
    "    \n",
    "    run += 1\n",
    "    \n",
    "utility_score['Clinical knowledge violation'] = np.array(male_code_violation) + np.array(female_code_violation)\n",
    "print('Clinical knowledge violation: male code violation ', male_code_violation, 'female code violation ', female_code_violation)\n",
    "print('Combined violation: ', np.array(male_code_violation) + np.array(female_code_violation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc6589",
   "metadata": {},
   "source": [
    "### 1.f TSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0873b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train on real training dataset and test on real testing data set\n",
    "\n",
    "random.seed(a=2021, version=2)\n",
    "\n",
    "RECALL_THRESHOLD = 0.6  # hold this to compute other threshold related metrics\n",
    "LABEL_INDEX = 6  # label column index in the dataset\n",
    "RACE_COL = [0,1,2,3,4,5]\n",
    "COL_LIST = list(real_data_df.columns)[len(RACE_COL)+1:]\n",
    "CAT_IDX_wo_RACE_label = list(np.linspace(0, len(COL_LIST) - 5, num=len(COL_LIST) - 4).astype(int))\n",
    "\n",
    "## Load the dataset for model training. Here it is real data. 70%\n",
    "train_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_training_data.csv').values.astype(np.float)\n",
    "train_real_data_index = np.linspace(0,len(train_real_data)-1,len(train_real_data)).astype('int')\n",
    "random.shuffle(train_real_data_index)\n",
    "train_real_data = train_real_data[train_real_data_index]\n",
    "train_real_label = train_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "train_real_data = np.delete(train_real_data, [LABEL_INDEX]+RACE_COL, axis = 1)\n",
    "print('The number of records in training (real) data is:  %d' % len(train_real_data))\n",
    "print('The number of features in training (real) data is:  %d' % len(train_real_data[0]))\n",
    "print('Positive vs Negative ratio in training (real) data is: %f' % (np.sum(train_real_label)/len(train_real_label)))\n",
    "\n",
    "## load the dataset for model evaluation. Here it is real data. 30%\n",
    "eval_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_testing_data.csv').values.astype(np.float)\n",
    "eval_real_label = eval_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "eval_real_data = np.delete(eval_real_data, [LABEL_INDEX]+RACE_COL, axis=1)\n",
    "print('\\nThe number of records in evaluation (real) data is: %d' % len(eval_real_data))\n",
    "print('The number of features in training (real) data is: %d' % len(eval_real_data[0]))\n",
    "print('Positive vs Negative ratio in evaluation (real) data is: %f' % (np.sum(eval_real_label)/len(eval_real_label)))\n",
    "\n",
    "## model training\n",
    "print('\\n !!!!!!!!!!!!!!!!!!! training is starting !!!!!!!!!!!!!!!!!!! ')\n",
    "\n",
    "gkf = KFold(n_splits=5, shuffle=True, random_state=0).split(X=train_real_data, y=train_real_label)\n",
    "\n",
    "\n",
    "# ############\n",
    "# # try some candidates here\n",
    "\n",
    "# param_grid = {\n",
    "#             'n_estimators': [500,1000],\n",
    "#             'colsample_bytree': [0.8,0.9],\n",
    "#             'max_depth': [15,20],\n",
    "#             'num_leaves': [50,80],\n",
    "#             'reg_alpha': [1.1, 1.3],\n",
    "#             'min_split_gain': [0.3, 0.5],\n",
    "#             'subsample': [0.6, 0.9],\n",
    "#             'subsample_freq': [20, 40]\n",
    "#             }\n",
    "############\n",
    "\n",
    "param_grid = {\n",
    "            'n_estimators': [500],\n",
    "            'colsample_bytree': [0.9],\n",
    "            'max_depth': [15],\n",
    "            'num_leaves': [50],\n",
    "            'reg_alpha': [1.3],\n",
    "            'min_split_gain': [0.3],\n",
    "            'subsample': [0.9],\n",
    "            'subsample_freq': [40]\n",
    "            }\n",
    "\n",
    "lgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=2000, learning_rate=0.01, metric='auc',categorical_feature=CAT_IDX_wo_RACE_label, n_jobs = 20)\n",
    "gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=gkf)\n",
    "gbm = gsearch.fit(X=train_real_data, y=train_real_label)\n",
    "print(\"Best parameters:\\n\")\n",
    "print(gbm.best_params_)\n",
    "\n",
    "y_scores = gbm.predict_proba(eval_real_data)\n",
    "y_scores = y_scores[:,1]\n",
    "trtr_7_3_auroc = roc_auc_score(y_score=y_scores, y_true=eval_real_label)\n",
    "trtr_prauc = average_precision_score(eval_real_label, y_scores)\n",
    "fpr, tpr, threshold_candidate = roc_curve(eval_real_label, y_scores)\n",
    "thres_index = (tpr > RECALL_THRESHOLD).tolist().index(True)\n",
    "thres = threshold_candidate[thres_index]\n",
    "print(\"Threshold for fixing recall as %.2f is %.4f\" % (RECALL_THRESHOLD, thres))\n",
    "pred_y = np.array([(value>=thres)*1.0 for value in y_scores])\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(eval_real_label, pred_y).ravel()\n",
    "trtr_ppv = tp / (tp + fp)\n",
    "trtr_npv = tn / (tn + fn)\n",
    "trtr_sens = tp / (tp + fn)\n",
    "trtr_spes = tn / (tn + fp)\n",
    "trtr_acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"      *** Test on real data AUROC: %.4f, PRAUC: %.4f, ACC: %.4f, PPV: %.4f, NPV: %.4f, Sensitivity: %.4f, Specificity: %.4f\" % (\n",
    "trtr_7_3_auroc, trtr_prauc, trtr_acc, trtr_ppv, trtr_npv, trtr_sens, trtr_spes))\n",
    "\n",
    "explainer = shap.TreeExplainer(gbm.best_estimator_)\n",
    "shap_df = pd.DataFrame(explainer.shap_values(eval_real_data)[1], columns = COL_LIST)\n",
    "shap_df_abs = abs(shap_df)\n",
    "\n",
    "feature_importance_mean = shap_df_abs.mean(axis=0).sort_values(ascending=False)\n",
    "feature_importance_value = {}\n",
    "for key, value in feature_importance_mean.items():\n",
    "    if key in feature_importance_value.keys():\n",
    "        feature_importance_value[key].append(value)\n",
    "    else:\n",
    "        feature_importance_value[key] = [value]\n",
    "\n",
    "correlation_coeff_value = {}\n",
    "for key in COL_LIST:\n",
    "    corr_value = np.corrcoef(shap_df[key], eval_real_data[:, COL_LIST.index(key)])[1][0]\n",
    "    if key in correlation_coeff_value.keys():\n",
    "        correlation_coeff_value[key].append(corr_value)\n",
    "    else:\n",
    "        correlation_coeff_value[key] = [corr_value]\n",
    "        \n",
    "np.save('./result/feature_importance_TRTR.npy', feature_importance_value)\n",
    "\n",
    "# np.save('./result/r_70_train_r_30_test_correl_coeff_value.npy', correlation_coeff_value)\n",
    "# shap_df.to_csv('./result/r_70_train_r_30_test_feature_importance.csv')\n",
    "# np.save('./result/r_70_train_r_30_test_y_estimate.npy', y_scores)\n",
    "# np.save('./result/r_70_train_r_30_test_y_label.npy', eval_real_label)\n",
    "# joblib.dump(gbm.best_estimator_, './result/r_70_train_r_30_test.pkl')\n",
    "\n",
    "print('TRTR AUROC: %.4f' % trtr_7_3_auroc)\n",
    "print('TRTR AUPRC: %.4f' % trtr_prauc)\n",
    "print('TRTR ACCURACY: %.4f' % trtr_acc)\n",
    "print('TRTR PPV: %.4f' % trtr_ppv)\n",
    "print('TRTR NPV: %.4f' % trtr_npv)\n",
    "print('TRTR RECALL: %.4f' % trtr_sens)\n",
    "print('TRTR SPESIFICITY: %.4f' % trtr_spes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b86b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4506526",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature importance plot\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 256)\n",
    "top = cm.get_cmap('Oranges_r', 128)\n",
    "bottom = cm.get_cmap('Blues', 128)\n",
    "newcolors = np.vstack((top(np.linspace(0, 1, 128)), bottom(np.linspace(0, 1, 128))))\n",
    "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
    "\n",
    "def ABS_SHAP(feature_importance_value, correlation_coeff_value, top, colors):\n",
    "\n",
    "    correlation_coeff_value_ = {}\n",
    "    for key, value in correlation_coeff_value.items():\n",
    "        correlation_coeff_value_[key] = np.mean(value)\n",
    "    corr_df = pd.DataFrame.from_dict(correlation_coeff_value_, orient='index').fillna(0)\n",
    "    corr_df.reset_index(inplace=True)\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    color_assigned = []\n",
    "    for corr in list(corr_df['Corr']):\n",
    "        if not np.isnan(corr):\n",
    "            color_assigned.append(colors[int((corr+1)/2 * len(colors))])\n",
    "        else:\n",
    "            color_assigned.append([1,1,1,1])\n",
    "    corr_df['Sign'] = color_assigned\n",
    "\n",
    "    feature_importance_value_ = {}\n",
    "    feature_importance_value_std = {}\n",
    "    for key, value in feature_importance_value.items():\n",
    "        feature_importance_value_[key] = np.mean(value)\n",
    "        feature_importance_value_std[key] = np.std(value)\n",
    "\n",
    "    feature_importance_std_df = pd.DataFrame.from_dict(feature_importance_value_std, orient='index').fillna(0)\n",
    "    feature_importance_std_df.reset_index(inplace=True)\n",
    "    feature_importance_std_df.columns = ['Variable','SHAP_std']\n",
    "\n",
    "    feature_importance_df = pd.DataFrame.from_dict(feature_importance_value_, orient='index').fillna(0)\n",
    "    feature_importance_df.reset_index(inplace=True)\n",
    "    feature_importance_df.columns = ['Variable','SHAP_abs']\n",
    "\n",
    "    feature_importance_df = feature_importance_df.merge(feature_importance_std_df, left_on='Variable',right_on='Variable',how='inner')\n",
    "\n",
    "    k2 = feature_importance_df.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = False).head(top).iloc[::-1]\n",
    "    print(list(k2['Variable']))\n",
    "    colorlist = k2['Sign']\n",
    "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(8,10),legend=False, xerr='SHAP_std', edgecolor='black')\n",
    "    ax.set_xlabel(\"Feature contribution [SHAP Values].\")\n",
    "    ax.set_ylabel(\"Factors\")\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.xaxis.grid(color='gray', linestyle='-.', linewidth=0.5)\n",
    "#     plt.savefig('feature_imp_0.png', dpi=300) \n",
    "#     plt.savefig('feature_imp_0.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    fig, aaxx = plt.subplots(figsize=(0.2, 15))\n",
    "    fig.subplots_adjust(bottom=0.5)\n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "    norm = matplotlib.colors.Normalize(vmin=-1, vmax=1)\n",
    "    cb1 = matplotlib.colorbar.ColorbarBase(aaxx, cmap=cmap,\n",
    "                                norm=norm, orientation='vertical')\n",
    "#     plt.savefig('feature_imp_1.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return k2\n",
    "\n",
    "ABS_SHAP(feature_importance_value, correlation_coeff_value, 32, newcolors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/YOUR_LOCAL_PATH/original_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d47ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train on synthetic dataset and test on real testing data set\n",
    "\n",
    "random.seed(a=2021, version=2)\n",
    "\n",
    "RECALL_THRESHOLD = 0.6  # hold this to compute other threshold related metrics\n",
    "LABEL_INDEX = 6  # label column index in the dataset\n",
    "RACE_COL = [0,1,2,3,4,5]\n",
    "COL_LIST = list(real_data_df.columns)[len(RACE_COL)+1:]\n",
    "CAT_IDX_wo_RACE_label = list(np.linspace(0, len(COL_LIST) - 5, num=len(COL_LIST) - 4).astype(int))\n",
    "## load the dataset for model evaluation. Here it is real data. 30%\n",
    "eval_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_testing_data.csv').values.astype(np.float)\n",
    "eval_real_label = eval_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "eval_real_data = np.delete(eval_real_data, RACE_COL + [LABEL_INDEX], axis=1)\n",
    "print('\\nThe number of records in evaluation (real) data is: %d' % len(eval_real_data))\n",
    "print('The number of features in training (real) data is: %d' % len(eval_real_data[0]))\n",
    "print('Positive vs Negative ratio in evaluation (real) data is: %f' % (np.sum(eval_real_label)/len(eval_real_label)))\n",
    "\n",
    "\n",
    "pred_result = []\n",
    "correlation_result = []\n",
    "\n",
    "real_auroc_result = []\n",
    "real_prauc_result = []\n",
    "real_acc_result = []\n",
    "real_ppv_result = []\n",
    "real_npv_result = []\n",
    "real_sens_result = []\n",
    "real_spes_result = []\n",
    "\n",
    "print('\\n   !!!!!!!!!!!!!!!!!!! training is starting !!!!!!!!!!!!!!!!!!! ')\n",
    "\n",
    "for run_ in range(len(model_id_list)):\n",
    "\n",
    "    run = run_ + 1\n",
    "    print('\\n ######## Syn dataset %d ######## ' % run)\n",
    "    syn_training_data = syn_data_list[run_]\n",
    "    random.seed(a=2021, version=2)\n",
    "    syn_training_data_index = np.linspace(0,len(syn_training_data)-1,len(syn_training_data)).astype('int')\n",
    "    random.shuffle(syn_training_data_index)\n",
    "    syn_training_data = syn_training_data[syn_training_data_index]\n",
    "    syn_training_data_label = syn_training_data[:,LABEL_INDEX].astype(np.float)\n",
    "    syn_training_data = np.delete(syn_training_data, [LABEL_INDEX]+RACE_COL, axis = 1)\n",
    "    \n",
    "    gkf = KFold(n_splits=5, shuffle=True, random_state=0).split(X=syn_training_data, y=syn_training_data_label)\n",
    "\n",
    "    param_grid = {\n",
    "            'n_estimators': [500],\n",
    "            'colsample_bytree': [0.9],\n",
    "            'max_depth': [15],\n",
    "            'num_leaves': [50],\n",
    "            'reg_alpha': [1.3],\n",
    "            'min_split_gain': [0.3],\n",
    "            'subsample': [0.9],\n",
    "            'subsample_freq': [40]\n",
    "            }\n",
    "\n",
    "    lgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=2000, learning_rate=0.01, metric='auc',categorical_feature=CAT_IDX_wo_RACE_label, n_jobs = 20)\n",
    "    gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=gkf)\n",
    "    gbm = gsearch.fit(X=syn_training_data, y=syn_training_data_label)\n",
    "    print(\"Best parameters:\\n\")\n",
    "    print(gbm.best_params_)\n",
    "\n",
    "    y_scores = gbm.predict_proba(eval_real_data)\n",
    "    y_scores = y_scores[:,1]\n",
    "    tstr_auroc = roc_auc_score(y_score=y_scores, y_true=eval_real_label)\n",
    "    tstr_prauc = average_precision_score(eval_real_label, y_scores)\n",
    "    fpr, tpr, threshold_candidate = roc_curve(eval_real_label, y_scores)\n",
    "    thres_index = (tpr > RECALL_THRESHOLD).tolist().index(True)\n",
    "    thres = threshold_candidate[thres_index]\n",
    "    print(\"Threshold for fixing recall as %.2f is %.4f\" % (RECALL_THRESHOLD, thres))\n",
    "    pred_y = np.array([(value>=thres)*1.0 for value in y_scores])\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(eval_real_label, pred_y).ravel()\n",
    "    tstr_ppv = tp / (tp + fp)\n",
    "    tstr_npv = tn / (tn + fn)\n",
    "    tstr_sens = tp / (tp + fn)\n",
    "    tstr_spes = tn / (tn + fp)\n",
    "    tstr_acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "    print(\"      *** Test on real data AUROC: %.4f, PRAUC: %.4f, ACC: %.4f, PPV: %.4f, NPV: %.4f, Sensitivity: %.4f, Specificity: %.4f\" % (\n",
    "    tstr_auroc, tstr_prauc, tstr_acc, tstr_ppv, tstr_npv, tstr_sens, tstr_spes))\n",
    "\n",
    "    real_auroc_result.append(tstr_auroc)\n",
    "    real_prauc_result.append(tstr_prauc)\n",
    "    real_acc_result.append(tstr_acc)\n",
    "    real_ppv_result.append(tstr_ppv)\n",
    "    real_npv_result.append(tstr_npv)\n",
    "    real_sens_result.append(tstr_sens)\n",
    "    real_spes_result.append(tstr_spes)\n",
    "\n",
    "    explainer = shap.TreeExplainer(gbm.best_estimator_)\n",
    "#         shap_value_list.extend(explainer.shap_values(eval_real_data)[1])\n",
    "    shap_df = pd.DataFrame(explainer.shap_values(eval_real_data)[1], columns = COL_LIST)\n",
    "    shap_df_abs = abs(shap_df)\n",
    "\n",
    "    feature_importance_mean = shap_df_abs.mean(axis=0).sort_values(ascending=False)\n",
    "    feature_importance_value = {}\n",
    "    for key, value in feature_importance_mean.items():\n",
    "        if key in feature_importance_value.keys():\n",
    "            feature_importance_value[key].append(value)\n",
    "        else:\n",
    "            feature_importance_value[key] = [value]\n",
    "            \n",
    "    np.save(f'./result/TSTR_feature_importance_run_{run}.npy', feature_importance_value)\n",
    "\n",
    "\n",
    "print('TSTR AUROC: ', real_auroc_result)\n",
    "utility_score['TSTR_auroc'] = real_auroc_result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0819102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916dcd5",
   "metadata": {},
   "source": [
    "## 1.g TRTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train on real testing dataset and test on real training data set\n",
    "\n",
    "random.seed(a=2021, version=2)\n",
    "\n",
    "RECALL_THRESHOLD = 0.6  # hold this to compute other threshold related metrics\n",
    "LABEL_INDEX = 6  # label column index in the dataset\n",
    "RACE_COL = [0,1,2,3,4,5]\n",
    "COL_LIST = list(real_data_df.columns)[len(RACE_COL)+1:]\n",
    "CAT_IDX_wo_RACE_label = list(np.linspace(0, len(COL_LIST) - 5, num=len(COL_LIST) - 4).astype(int))\n",
    "\n",
    "## Load the dataset for model training. Here it is real data. 70%\n",
    "train_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_testing_data.csv').values.astype(np.float)\n",
    "train_real_data_index = np.linspace(0,len(train_real_data)-1,len(train_real_data)).astype('int')\n",
    "random.shuffle(train_real_data_index)\n",
    "train_real_data = train_real_data[train_real_data_index]\n",
    "train_real_label = train_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "train_real_data = np.delete(train_real_data, [LABEL_INDEX]+RACE_COL, axis = 1)\n",
    "print('The number of records in training (real) data is:  %d' % len(train_real_data))\n",
    "print('The number of features in training (real) data is:  %d' % len(train_real_data[0]))\n",
    "print('Positive vs Negative ratio in training (real) data is: %f' % (np.sum(train_real_label)/len(train_real_label)))\n",
    "\n",
    "## load the dataset for model evaluation. Here it is real data. 30%\n",
    "eval_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_training_data.csv').values.astype(np.float)\n",
    "eval_real_label = eval_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "eval_real_data = np.delete(eval_real_data, [LABEL_INDEX]+RACE_COL, axis=1)\n",
    "print('\\nThe number of records in evaluation (real) data is: %d' % len(eval_real_data))\n",
    "print('The number of features in training (real) data is: %d' % len(eval_real_data[0]))\n",
    "print('Positive vs Negative ratio in evaluation (real) data is: %f' % (np.sum(eval_real_label)/len(eval_real_label)))\n",
    "\n",
    "## model training\n",
    "print('\\n !!!!!!!!!!!!!!!!!!! training is starting !!!!!!!!!!!!!!!!!!! ')\n",
    "\n",
    "gkf = KFold(n_splits=5, shuffle=True, random_state=0).split(X=train_real_data, y=train_real_label)\n",
    "\n",
    "\n",
    "# ############\n",
    "# # try some candidates here\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [500, 1000],\n",
    "#     'colsample_bytree': [0.7, 0.8],\n",
    "#     'max_depth': [15, 25],\n",
    "#     'num_leaves': [20, 50],\n",
    "#     'reg_alpha': [1.1, 1.3],\n",
    "# #     'reg_lambda': [1.1, 1.3],\n",
    "#     'min_split_gain': [0.3, 0.5],\n",
    "#     'subsample': [0.8, 0.9],\n",
    "#     'subsample_freq': [20]\n",
    "#     }\n",
    "############\n",
    "\n",
    "param_grid = {\n",
    "            'n_estimators': [500],\n",
    "            'colsample_bytree': [0.9],\n",
    "            'max_depth': [15],\n",
    "            'num_leaves': [50],\n",
    "            'reg_alpha': [1.3],\n",
    "            'min_split_gain': [0.3],\n",
    "            'subsample': [0.9],\n",
    "            'subsample_freq': [40]\n",
    "            }\n",
    "\n",
    "lgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=2000, learning_rate=0.01, metric='auc',categorical_feature=CAT_IDX_wo_RACE_label, n_jobs = 20)\n",
    "gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=gkf)\n",
    "gbm = gsearch.fit(X=train_real_data, y=train_real_label)\n",
    "print(\"Best parameters:\\n\")\n",
    "print(gbm.best_params_)\n",
    "\n",
    "y_scores = gbm.predict_proba(eval_real_data)\n",
    "y_scores = y_scores[:,1]\n",
    "trtr_auroc = roc_auc_score(y_score=y_scores, y_true=eval_real_label)\n",
    "trtr_prauc = average_precision_score(eval_real_label, y_scores)\n",
    "fpr, tpr, threshold_candidate = roc_curve(eval_real_label, y_scores)\n",
    "thres_index = (tpr > RECALL_THRESHOLD).tolist().index(True)\n",
    "thres = threshold_candidate[thres_index]\n",
    "print(\"Threshold for fixing recall as %.2f is %.4f\" % (RECALL_THRESHOLD, thres))\n",
    "pred_y = np.array([(value>=thres)*1.0 for value in y_scores])\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(eval_real_label, pred_y).ravel()\n",
    "trtr_ppv = tp / (tp + fp)\n",
    "trtr_npv = tn / (tn + fn)\n",
    "trtr_sens = tp / (tp + fn)\n",
    "trtr_spes = tn / (tn + fp)\n",
    "trtr_acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"      *** Test on real data AUROC: %.4f, PRAUC: %.4f, ACC: %.4f, PPV: %.4f, NPV: %.4f, Sensitivity: %.4f, Specificity: %.4f\" % (\n",
    "trtr_auroc, trtr_prauc, trtr_acc, trtr_ppv, trtr_npv, trtr_sens, trtr_spes))\n",
    "\n",
    "explainer = shap.TreeExplainer(gbm.best_estimator_)\n",
    "shap_df = pd.DataFrame(explainer.shap_values(eval_real_data)[1], columns = COL_LIST)\n",
    "shap_df_abs = abs(shap_df)\n",
    "\n",
    "feature_importance_mean = shap_df_abs.mean(axis=0).sort_values(ascending=False)\n",
    "feature_importance_value = {}\n",
    "for key, value in feature_importance_mean.items():\n",
    "    if key in feature_importance_value.keys():\n",
    "        feature_importance_value[key].append(value)\n",
    "    else:\n",
    "        feature_importance_value[key] = [value]\n",
    "\n",
    "correlation_coeff_value = {}\n",
    "for key in COL_LIST:\n",
    "    corr_value = np.corrcoef(shap_df[key], eval_real_data[:, COL_LIST.index(key)])[1][0]\n",
    "    if key in correlation_coeff_value.keys():\n",
    "        correlation_coeff_value[key].append(corr_value)\n",
    "    else:\n",
    "        correlation_coeff_value[key] = [corr_value]\n",
    "\n",
    "# np.save('./result/r_70_train_r_30_test_correl_coeff_value.npy', correlation_coeff_value)\n",
    "# shap_df.to_csv('./result/r_70_train_r_30_test_feature_importance.csv')\n",
    "# np.save('./result/r_70_train_r_30_test_y_estimate.npy', y_scores)\n",
    "# np.save('./result/r_70_train_r_30_test_y_label.npy', eval_real_label)\n",
    "# joblib.dump(gbm.best_estimator_, './result/r_70_train_r_30_test.pkl')\n",
    "\n",
    "print('TRTR AUROC: %.4f' % trtr_auroc)\n",
    "print('TRTR AUPRC: %.4f' % trtr_prauc)\n",
    "print('TRTR ACCURACY: %.4f' % trtr_acc)\n",
    "print('TRTR PPV: %.4f' % trtr_ppv)\n",
    "print('TRTR NPV: %.4f' % trtr_npv)\n",
    "print('TRTR RECALL: %.4f' % trtr_sens)\n",
    "print('TRTR SPESIFICITY: %.4f' % trtr_spes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973339b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train on real testing dataset and test on synthetic training data set\n",
    "\n",
    "random.seed(a=2021, version=2)\n",
    "\n",
    "train_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_testing_data.csv').values.astype(np.float)\n",
    "train_real_data_index = np.linspace(0,len(train_real_data)-1,len(train_real_data)).astype('int')\n",
    "random.shuffle(train_real_data_index)\n",
    "train_real_data = train_real_data[train_real_data_index]\n",
    "train_real_label = train_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "train_real_data = np.delete(train_real_data, [LABEL_INDEX]+RACE_COL, axis = 1)\n",
    "print('The number of records in training (real) data is:  %d' % len(train_real_data))\n",
    "print('The number of features in training (real) data is:  %d' % len(train_real_data[0]))\n",
    "print('Positive vs Negative ratio in training (real) data is: %f' % (np.sum(train_real_label)/len(train_real_label)))\n",
    "\n",
    "gkf = KFold(n_splits=5, shuffle=True, random_state=0).split(X=train_real_data, y=train_real_label)\n",
    "\n",
    "param_grid = {\n",
    "            'n_estimators': [500],\n",
    "            'colsample_bytree': [0.9],\n",
    "            'max_depth': [15],\n",
    "            'num_leaves': [50],\n",
    "            'reg_alpha': [1.3],\n",
    "            'min_split_gain': [0.3],\n",
    "            'subsample': [0.9],\n",
    "            'subsample_freq': [40]\n",
    "            }\n",
    "\n",
    "lgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=2000, learning_rate=0.01, metric='auc',categorical_feature=CAT_IDX_wo_RACE_label, n_jobs = 20)\n",
    "\n",
    "gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=gkf)\n",
    "gbm = gsearch.fit(X=train_real_data, y=train_real_label)\n",
    "print(\"Best parameters:\\n\")\n",
    "print(gbm.best_params_)\n",
    "\n",
    "print('\\n   !!!!!!!!!!!!!!!!!!! evaluation is starting !!!!!!!!!!!!!!!!!!! ')\n",
    "\n",
    "syn_auroc_result = []\n",
    "syn_prauc_result = []\n",
    "syn_acc_result = []\n",
    "syn_ppv_result = []\n",
    "syn_npv_result = []\n",
    "syn_sens_result = []\n",
    "syn_spes_result = []\n",
    "\n",
    "for run_ in range(len(model_id_list)):\n",
    "\n",
    "    run = run_ + 1\n",
    "    print('\\n ######## Syn dataset %d ######## ' % run)\n",
    "    \n",
    "    syn_data = syn_data_list[run_]\n",
    "    random.seed(a=2021, version=2)\n",
    "    syn_data_index = np.linspace(0,len(syn_data)-1,len(syn_data)).astype('int')\n",
    "    random.shuffle(syn_data_index)\n",
    "    syn_data = syn_data[syn_data_index]\n",
    "    syn_label = syn_data[:,LABEL_INDEX].astype(np.float)\n",
    "    syn_data = np.delete(syn_data, [LABEL_INDEX]+RACE_COL, axis = 1)\n",
    "\n",
    "    y_scores = gbm.best_estimator_.predict_proba(syn_data)\n",
    "    y_scores = y_scores[:,1]\n",
    "    auroc = roc_auc_score(y_score=y_scores, y_true=syn_label)\n",
    "    prauc = average_precision_score(syn_label, y_scores)\n",
    "    fpr, tpr, threshold_candidate = roc_curve(syn_label, y_scores)\n",
    "    thres_index = (tpr > RECALL_THRESHOLD).tolist().index(True)\n",
    "    thres = threshold_candidate[thres_index]\n",
    "    print(\"Threshold for fixing recall as %.2f is %.4f\" % (RECALL_THRESHOLD, thres))\n",
    "    pred_y = np.array([(value>=thres)*1.0 for value in y_scores])\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(syn_label, pred_y).ravel()\n",
    "    ppv = tp / (tp + fp)\n",
    "    npv = tn / (tn + fn)\n",
    "    sens = tp / (tp + fn)\n",
    "    spes = tn / (tn + fp)\n",
    "    acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "    print(\"      *** Test on syn data AUROC: %.4f, PRAUC: %.4f, ACC: %.4f, PPV: %.4f, NPV: %.4f, Sensitivity: %.4f, Specificity: %.4f\" % (\n",
    "    auroc, prauc, acc, ppv, npv, sens, spes))   \n",
    "\n",
    "    syn_auroc_result.append(auroc)\n",
    "    syn_prauc_result.append(prauc)\n",
    "    syn_acc_result.append(acc)\n",
    "    syn_ppv_result.append(ppv)\n",
    "    syn_npv_result.append(npv)\n",
    "    syn_sens_result.append(sens)\n",
    "    syn_spes_result.append(spes)\n",
    "\n",
    "\n",
    "print('TRTS AUROC: ', syn_auroc_result)\n",
    "utility_score['TRTS_auroc'] = syn_auroc_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5405ee",
   "metadata": {},
   "source": [
    "## 1.h feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610307d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the real dataset for model training.\n",
    "\n",
    "random.seed(a=2021, version=2)\n",
    "\n",
    "RECALL_THRESHOLD = 0.6  # hold this to compute other threshold related metrics\n",
    "LABEL_INDEX = 6  # label column index in the dataset\n",
    "RACE_COL = [0,1,2,3,4,5]\n",
    "# CAT_IDX_wo_RACE_label = list(np.linspace(0, len(COL_LIST) - 5, num=len(COL_LIST) - 4).astype(int))\n",
    "\n",
    "\n",
    "feature_importance_TRTR = np.load('./result/feature_importance_TRTR.npy', allow_pickle=True).item()\n",
    "COL_LIST = list(real_data_df.columns)[len(RACE_COL)+1:]\n",
    "top_feature_indices = [COL_LIST.index(name) for name in list(feature_importance_TRTR.keys())]\n",
    "cat_or_not = [0,1,1,1,1,0,0] + [1]*18 + [0] + [1]*(len(COL_LIST)-26) # indicate mannually the binary features\n",
    "\n",
    "train_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_training_data.csv').values.astype(np.float)\n",
    "train_real_data_index = np.linspace(0,len(train_real_data)-1,len(train_real_data)).astype('int')\n",
    "random.shuffle(train_real_data_index)\n",
    "train_real_data = train_real_data[train_real_data_index]\n",
    "train_real_label = train_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "train_real_data = np.delete(train_real_data, [LABEL_INDEX]+RACE_COL, axis = 1)\n",
    "print('The number of records in training (real) data is:  %d' % len(train_real_data))\n",
    "print('The number of features in training (real) data is:  %d' % len(train_real_data[0]))\n",
    "print('Positive vs Negative ratio in training (real) data is: %f' % (np.sum(train_real_label)/len(train_real_label)))\n",
    "\n",
    "## load the dataset for model evaluation. Here it is real data. 30%\n",
    "eval_real_data = pd.read_csv('/YOUR_LOCAL_PATH/original_testing_data.csv').values.astype(np.float)\n",
    "eval_real_label = eval_real_data[:,LABEL_INDEX].astype(np.float)\n",
    "eval_real_data = np.delete(eval_real_data, [LABEL_INDEX]+RACE_COL, axis=1)\n",
    "print('\\nThe number of records in evaluation (real) data is: %d' % len(eval_real_data))\n",
    "print('The number of features in training (real) data is: %d' % len(eval_real_data[0]))\n",
    "print('Positive vs Negative ratio in evaluation (real) data is: %f' % (np.sum(eval_real_label)/len(eval_real_label)))\n",
    "\n",
    "## model training\n",
    "print('\\n !!!!!!!!!!!!!!!!!!! training is starting !!!!!!!!!!!!!!!!!!! ')\n",
    "\n",
    "test_candidates = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "auroc_performance = {}\n",
    "auprc_performance = {}\n",
    "\n",
    "for num_features in test_candidates:\n",
    "    print('Testing top %d features: ' % num_features)\n",
    "\n",
    "    selected_feature_indices = top_feature_indices[:num_features]\n",
    "    selected_feature_cat_or_not = cat_or_not[:num_features]\n",
    "    cat_indices = [i for i in range(len(selected_feature_cat_or_not)) if selected_feature_cat_or_not[i] == 1]\n",
    "    \n",
    "    gkf = KFold(n_splits=5, shuffle=True, random_state=0).split(X=train_real_data, y=train_real_label)\n",
    "\n",
    "    param_grid = {\n",
    "            'n_estimators': [500],\n",
    "            'colsample_bytree': [0.9],\n",
    "            'max_depth': [15],\n",
    "            'num_leaves': [50],\n",
    "            'reg_alpha': [1.3],\n",
    "            'min_split_gain': [0.3],\n",
    "            'subsample': [0.9],\n",
    "            'subsample_freq': [40]\n",
    "            }\n",
    "\n",
    "    lgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=2000, learning_rate=0.01, metric='auc',categorical_feature=cat_indices, n_jobs = 20)\n",
    "\n",
    "    gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=gkf)\n",
    "    gbm = gsearch.fit(X=train_real_data[:, selected_feature_indices], y=train_real_label)\n",
    "    print(\"Best parameters:\\n\")\n",
    "    print(gbm.best_params_)\n",
    "\n",
    "    y_scores = gbm.predict_proba(eval_real_data[:, selected_feature_indices])\n",
    "    y_scores = y_scores[:,1]\n",
    "    auroc = roc_auc_score(y_score=y_scores, y_true=eval_real_label)\n",
    "    prauc = average_precision_score(eval_real_label, y_scores)\n",
    "    fpr, tpr, threshold_candidate = roc_curve(eval_real_label, y_scores)\n",
    "    thres_index = (tpr > RECALL_THRESHOLD).tolist().index(True)\n",
    "    thres = threshold_candidate[thres_index]\n",
    "    print(\"      Threshold for fixing recall as %.2f is %.4f\" % (RECALL_THRESHOLD, thres))\n",
    "    pred_y = np.array([(value>=thres)*1.0 for value in y_scores])\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(eval_real_label, pred_y).ravel()\n",
    "    ppv = tp / (tp + fp)\n",
    "    npv = tn / (tn + fn)\n",
    "    sens = tp / (tp + fn)\n",
    "    spes = tn / (tn + fp)\n",
    "    acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "    print(\"      *** Test on real data AUROC: %.4f, PRAUC: %.4f, ACC: %.4f, PPV: %.4f, NPV: %.4f, Sensitivity: %.4f, Specificity: %.4f\" % (\n",
    "    auroc, prauc, acc, ppv, npv, sens, spes))\n",
    "    \n",
    "    auroc_performance[num_features] = auroc\n",
    "    auprc_performance[num_features] = prauc\n",
    "    \n",
    "np.save('./result/auroc_performance_feature_imp.npy', auroc_performance)\n",
    "np.save('./result/auprc_performance_feature_imp.npy', auprc_performance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f05b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal', 'size'  : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.figure(figsize=(7, 7)) \n",
    "plt.xlabel(\"Top features to include in training\", fontsize=20)\n",
    "plt.ylabel(\"AUROC\", fontsize=20)\n",
    "plt.ylim(0.8, 1.0)\n",
    "df = pd.DataFrame(auroc_performance.items(), columns=['x_axis', 'y_axis'])\n",
    "plt.plot('x_axis', 'y_axis', data=df, linestyle='-', marker='o')\n",
    "plt.plot([0, 100], [trtr_7_3_auroc, trtr_7_3_auroc], linestyle=':')  ## the performance of training on 70% real data and testing on 30% testing data\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23736ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, auroc in auroc_performance.items():\n",
    "    print(n, auroc, auroc/trtr_7_3_auroc)\n",
    "\n",
    "## select 95% performance as the threshold, thus 20 is the number of important features to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab985fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_score = []\n",
    "TRTR_top_features = set(list(feature_importance_TRTR.keys())[:20])\n",
    "for run in range(len(syn_data_list)):\n",
    "    run += 1\n",
    "    feature_importance_value = np.load(f'./result/TSTR_feature_importance_run_{run}.npy', allow_pickle=True).item()\n",
    "    syn_top_imp_features = []\n",
    "    for key, imp in feature_importance_value.items():\n",
    "        syn_top_imp_features.append(key)\n",
    "        if len(syn_top_imp_features) == 20:\n",
    "            syn_top_imp_features = set(syn_top_imp_features)\n",
    "            intersection_imp_features = TRTR_top_features.intersection(syn_top_imp_features)\n",
    "            feature_imp_score.append(len(intersection_imp_features)/len(TRTR_top_features))\n",
    "            break\n",
    "utility_score['Feature importance'] = feature_imp_score\n",
    "feature_imp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./result/utility_score.npy', utility_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec0061",
   "metadata": {},
   "source": [
    "## 2. Rank datasets and derive scores for selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ranks = {}\n",
    "for metric, score in utility_score.items():\n",
    "    if metric not in ['TSTR_auroc', 'TRTS_auroc', 'Feature importance']:\n",
    "        ranks = ss.rankdata(score).astype(int)\n",
    "        dataset_ranks[metric] = list(ranks)\n",
    "    else:\n",
    "        ranks = len(syn_data_list) + 1 - ss.rankdata(score).astype(int)\n",
    "        dataset_ranks[metric] = list(ranks)\n",
    "    print(metric, ranks)\n",
    "np.save('./result/utility_score_dataset_ranks.npy', dataset_ranks)\n",
    "dataset_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040747aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
